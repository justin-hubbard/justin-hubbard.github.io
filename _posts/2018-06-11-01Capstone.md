---
layout: post
title: "Assistive Wheelchair Technologies for ALS Patients"
date:   2018-09-1 20:29:18 -0100
tags: code
---

#### For my Senior Design Project at Washington State University, I and four others took on the continuation of the Autonomous Wheelchair project, supported by Microsoft and Team Gleason.

![Poster for the End of Year Poster Session](/assets/images/Eris-Poster.png)

The main goal of this then three-year project was to take a standard assistive wheelchair and explore ways
that adding technology to it could improve the lives of people living with ALS. People with ALS can have a wide range of mobility levels ranging from mild impairment that allows for use of keyboards and joysticks to total impairment leaving only eye movement, so we had to be creative
with our ideas.

**With the help of our faculty mentors, we landed on three ideas to implement:**
- keyboard typing interfaces using Tobii eye-tracking
- Kinect and sonar-based object sensing
- Programmatic motor calibration. 

These requirements were reached over the course of two semesters of trial and error, constantly reassessing
how realistic or feasible our ideas were. Out of my interest in computer vision and sensors generally, I was
able to work on the Kinect/Sonar components along with one other teammate.

Previous years had hooked the chair up to
an Arduino for controling the motors and added an Intel NUC with a screen for more power intensive software. They did some work
with navigation using sensors mounted on the walls, and also made some progress with door detection using a Kinect, but
we decided to move on to more general solutions.  

When we initially tested the object detection implementation from the previous year, it was giving us false
positives almost exclusively. Not great. So we changed approaches and focused on depth sensing. Luckily, the
Kinect provides DepthFrames, so I was able to combine those with the actual sonar sensors on the chair to
create an array of depth sensors.

{% highlight csharp %}
BaseFrame frame = Data.BackgroundStream.GetFrame();
DFrame df = (DFrame)Data.BackgroundStream.GetFrame();

if (null != df)
{
    int dfHeight = df.GetHeight();
    int dfWidth = df.GetWidth();

    ushort[] dd = df.get_DepthData();

    int[] kDepths = new int[10];
    int[][] testD = new int[10][];

    for (int z = 0; z < 10; z++)
    {
        testD[z] = new int[dfHeight];
    }

    for (int i = 0; i < dfHeight; i++) // Loop over each column of the array
    {
        for (int j = 1; j < 11; j++)
        {
            int pixelVal = (int)dd[(dfWidth * i) + (j * (dfWidth / 11))];

            if (pixelVal == 0)
                testD[j-1][i] = 10000;
            else
                testD[j-1][i] = (int)dd[(dfWidth * i) + (j * (dfWidth / 11))];
        }
    }

    for (int k = 0; k < 10; k++)
    {
        int low = 10000; // testD[k][0];

        for (int l = 0; l < dfHeight; l++)
        {
            if (testD[k][l] < low)
                low = testD[k][l];
        }

        if (low == 10000)
            low = 100;

        depthPoints[13-(k+2)] = low / 10;
    }
{% endhighlight %}


This section of code takes a sample of the Kinect DepthFrames, takes vertical slices, and finds the smallest values (indicating the closest object sensed) while filtering out false zeroes and other bad data.

The depth slices are then combined with sonar data and passed to my partner's code that decides when to stop the chair from driving because it is about to hit something, as well as drawing a visual representation on a WinForms page I implemented:  

  



![Sonar Display](/assets/images/monitor.JPG)

This may not look like much, but it updates in realtime and gives a really clear picture of what the sensors
are seeing. This monitor is attached to an articulating arm on the wheelchair, so testing this was easy. Here's my partner Max running some tests:

<div style="width: 100%; margin: auto auto 10% auto;">
	<div class="ytcontainer">
		<iframe class="ytframe" src="http://www.youtube.com/embed/FQMp4DiPP_E"
		 frameborder="0" allowfullscreen></iframe>
	</div>
</div>

We spent a lot of time driving toward walls and garbage cans, hoping the chair would decide not to run into them. By the end of the semester, the chair was reliably not running into anything. I did run myself over once, but that's not important.

In the end, this was a project that taught me how to work in a team, which I suspect is probably the secret purpose of senior projects. It's like a trojan horse for developing valuable people
skills.

Although we had the same five-person team that every other group had, we ended up in a unique 
situation where we split up into three subteams for the three different ideas I mentioned earlier.
This required a lot of communication to make sure that our very separate components were on parallel paths to our ultimate goal.

Within our team, we had two guys for whom English was their second language, another whose social skills occasionally created difficulty with communicating clearly, and we all had different skillsets that we had to figure out and leverage how we could. On top of our group dynamics, we also had advisers who, espcially considering the exploratory nature of our project, were constantly changing our requirements as we reported our progress every week.


By the end of our project, we didn't have our communication perfect, our collaboration wasn't seamless, and I couldn't convince everyone to use Slack properly, but we delivered what we were supposed to deliver, and learned a lot on the way. 



[Github Project Repo](http://www.github.com)

























